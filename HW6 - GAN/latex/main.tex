\documentclass[a4paper]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{paralist}
\usepackage{epstopdf}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyvrb}
\usepackage{float}
\usepackage{paralist}
\usepackage[svgname]{xcolor}
\usepackage{enumerate}
\usepackage{array}
\usepackage{times}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage{environ}
\usepackage{times}
\usepackage{textcomp}
\usepackage{caption}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{subcaption}


\urlstyle{rm}

\setlength\parindent{0pt} % Removes all indentation from paragraphs
\theoremstyle{definition}
\newtheorem{definition}{Definition}[]
\newtheorem{conjecture}{Conjecture}[]
\newtheorem{example}{Example}[]
\newtheorem{theorem}{Theorem}[]
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\floatname{algorithm}{Procedure}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\Nat}{\mathbb{N}}
\newcommand{\br}[1]{\{#1\}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\renewcommand{\qedsymbol}{$\blacksquare$}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

\newcommand{\vc}[1]{\boldsymbol{#1}}
\newcommand{\xv}{\vc{x}}
\newcommand{\Sigmav}{\vc{\Sigma}}
\newcommand{\alphav}{\vc{\alpha}}
\newcommand{\muv}{\vc{\mu}}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\def\x{\mathbf x}
\def\y{\mathbf y}
\def\w{\mathbf w}
\def\v{\mathbf v}
\def\E{\mathbb E}
\def\V{\mathbb V}
\def\ind{\mathbbm 1}

% TO SHOW SOLUTIONS, include following (else comment out):
\newenvironment{soln}{
	\leavevmode\color{blue}\ignorespaces
}{}

\hypersetup{
	%    colorlinks,
	linkcolor={red!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}

\geometry{
	top=1in,            % <-- you want to adjust this
	inner=1in,
	outer=1in,
	bottom=1in,
	headheight=3em,       % <-- and this
	headsep=2em,          % <-- and this
	footskip=3em,
}


\pagestyle{fancyplain}
\lhead{\fancyplain{}{Homework 6}}
\rhead{\fancyplain{}{CS 760 Machine Learning}}
\cfoot{\thepage}

\title{\textsc{Homework 6}} % Title

%%% NOTE:  Replace 'NAME HERE' etc., and delete any "\red{}" wrappers (so it won't show up as red)

\author{
	\red{Lucas Poon} \\
	\red{llpoon}\\
} 

\date{}

\begin{document}
	
	\maketitle 
	
        \textbf{Instructions:}
        Use this latex file as a template to develop your homework. Submit your homework on time as a single pdf file. Please wrap your code and upload to a public GitHub repo, then attach the link below the instructions so that we can access it. Answers to the questions that are not within the pdf are not accepted. This includes external links or answers attached to the code implementation. Late submissions may not be accepted. You can choose any programming language (i.e. python, R, or MATLAB). Please check Piazza for updates about the homework. It is ok to share the results of the experiments and compare them with each other.
        \vspace{0.1in}
	
	\section{Implementation: GAN (50 pts)}
	In this part, you are expected to implement GAN with MNIST dataset. We have provided a base jupyter notebook (gan-base.ipynb) for you to start with, which provides a model setup and training configurations to train GAN with MNIST dataset.
	
	\begin{enumerate} [label=(\alph*)]
		\item Implement training loop and report learning curves and generated images in epoch 1, 50, 100. Note that drawing learning curves and visualization of images are already implemented in provided jupyter notebook. \hfill (20 pts)
		
		\begin{algorithm}
			\caption{Training GAN, modified from \cite{goodfellow2014generative}}\label{alg:GAN}
			\begin{algorithmic}
				\Require $m$: real data batch size, $n_{z}$: fake data batch size
				\Ensure Discriminator $D$, Generator $G$
				
				\For{number of training iterations}
				
				\State{$\#$ Training discriminator}
				\State{Sample minibatch of $n_{z}$ noise samples $\{z^{(1)}, z^{(2)}, \cdots, z^{(n_{z})}\}$ from noise prior $p_{g}(z)$}
				\State{Sample minibatch of $\{x^{(1)}, x^{(2)}, \cdots, x^{(m)}\}$}
				\State{Update the discriminator by ascending its stochastic  gradient:
					$$\nabla_{\theta_{d}} \big ( \cfrac{1}{m}  \sum_{i=1}^{m}  \log D(x^{(i)})  + \cfrac{1}{n_{z}} \sum_{i=1}^{n_{z}}  \log (1-D(G(z^{(i)})))\big )$$
				}
				
				\State{$\#$ Training generator}
				\State{Sample minibatch of $n_{z}$ noise samples $\{z^{(1)}, z^{(2)}, \cdots, z^{(n_{z})}\}$ from noise prior $p_{g}(z)$}
				\State{Update the generator  by ascending its stochastic  gradient:
					$$\nabla_{\theta_{g}}  \cfrac{1}{n_{z}} \sum_{i=1}^{n_{z}}  \log D(G(z^{(i)}))\big )$$
				}
				\EndFor
				
				\State{$\#$ The gradient-based updates can use any standard gradient-based learning rule. In the base code, we are using Adam optimizer \citep{kingma2014adam}}
			\end{algorithmic}
		\end{algorithm}
		
		Expected results are as follows.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.7\textwidth]{gan_q1_loss.png}
			\caption{Learning curve}
			\label{fig:gan_q1_loss}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{gan_q1_epoch1.png}
				\caption{epoch 1}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{gan_q1_epoch50.png}
				\caption{epoch 50}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{gan_q1_epoch100.png}
				\caption{epoch 100}
			\end{subfigure}
			\caption{Generated images by $G$}
			\label{fig:three graphs}
		\end{figure}
		
		\newpage
		\begin{soln}
        My Results:
        \begin{figure}[H]
			\centering
			\includegraphics[width=0.7\textwidth]{images/q1a_loss.png}
			\caption{Learning curve}
			\label{fig:gan_q1_loss}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{images/q1a_gen_img1.png}
				\caption{epoch 1}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{images/q1a_gen_img50.png}
				\caption{epoch 50}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{images/q1a_gen_img100.png}
				\caption{epoch 100}
			\end{subfigure}
			\caption{Generated images by $G$}
			\label{fig:three graphs}
		\end{figure}
        \end{soln}
		
		
		\newpage
		\item Replace the generator update rule as the original one in the slide,\\
		``Update the generator by descending its stochastic gradient:
		
		$$\nabla_{\theta_{g}}  \cfrac{1}{n_{z}}  \sum_{i=1}^{n_{z}}\log (1-D(G(z^{(i)})))\big )$$
		"
		, and report learning curves and generated images in epoch 1, 50, 100. Compare the result with (a). Note that it may not work. If training does not work, explain why it doesn't work. \\
        You may find this helpful: https://jonathan-hui.medium.com/gan-what-is-wrong-with-the-gan-cost-function-6f594162ce01
		\hfill (10 pts)
		
		\begin{soln}
        My Results:
        \begin{figure}[H]
			\centering
			\includegraphics[width=0.7\textwidth]{images/q1b_loss.png}
			\caption{Learning curve}
			\label{fig:gan_q1_loss}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{images/q1b_gen_img1.png}
				\caption{epoch 1}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{images/q1b_gen_img50.png}
				\caption{epoch 50}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{images/q1b_gen_img100.png}
				\caption{epoch 100}
			\end{subfigure}
			\caption{Generated images by $G$}
			\label{fig:three graphs}
		\end{figure}

        The training does not work because the gradient vanishes. The stochastic gradient $\nabla_{\theta_{g}}  \cfrac{1}{n_{z}}  \sum_{i=1}^{n_{z}}\log (1-D(G(z^{(i)})))\big )$ decreases as the discriminator improves, and eventually ends up vanishing when the discriminator becomes optimal.
        \end{soln}
		
		\item Except the method that we used in (a), how can we improve training for GAN? Implement that and report your setup, learning curves, and generated images in epoch 1, 50, 100.
        This question is an open-ended question and you can choose whichever method you want.
		\hfill (20 pts)
		
		\begin{soln} 
        We can also improve training for GAN by adding noise. The setup is exactly the same as the default, but I have added a noise (0.05 * torch.randn\_like(data)) to the real and fake data. \\
        My Results:
        \begin{figure}[H]
			\centering
			\includegraphics[width=0.7\textwidth]{images/q1c_loss.png}
			\caption{Learning curve}
			\label{fig:gan_q1_loss}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{images/q1c_gen_img1.png}
				\caption{epoch 1}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{images/q1c_gen_img50.png}
				\caption{epoch 50}
			\end{subfigure}
			\hfill
			\begin{subfigure}[b]{0.3\textwidth}
				\centering
				\includegraphics[width=\textwidth]{images/q1c_gen_img100.png}
				\caption{epoch 100}
			\end{subfigure}
			\caption{Generated images by $G$}
			\label{fig:three graphs}
		\end{figure}
        \end{soln}
		
	\end{enumerate}

\newpage
\section{Directed Graphical Model [25 points]}
Consider the directed graphical model (aka Bayesian network) in Figure~\ref{fig:bn}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{BN.jpeg}
    \caption{A Bayesian Network example.}
    \label{fig:bn}
\end{figure}
Compute $P(B=t \mid E=f,J=t,M=t)$ and $P(B=t \mid E=t,J=t,M=t)$. (10 points for each) These are the conditional probabilities of a burglar in your house (yikes!) when both of your neighbors John and Mary call you and say they hear an alarm in your house, but without or with an earthquake also going on in that area (what a busy day), respectively.

\begin{soln}
\begin{equation*}
\begin{split}
P(B=t \mid E=f,J=t,M=t) &= P(B \mid \neg E,J,M) \\
&= \cfrac{P(B, \neg E, J, M)}{P(\neg E, J, M)} \\
&= \cfrac{P(B, \neg E, J, M)}{P(B, \neg E, J, M) + P(\neg B, \neg E, J, M)} \\
\end{split}
\end{equation*}
$$P(B, \neg E, J, M) = P(B)P(\neg E) P(A \mid B, \neg E)P(J \mid A)P(M \mid A)$$
$$+ P(B)P(\neg E) P(\neg A \mid B, \neg E)P(J \mid \neg A)P(M \mid \neg A)$$
$$= (0.1)(0.8)(0.8)(0.9)(0.7) + (0.1)(0.8)(0.2)(0.2)(0.1) = 0.04064$$
$$P(\neg B, \neg E, J, M) = P(\neg B)P(\neg E) P(A \mid \neg B, \neg E)P(J \mid A)P(M \mid A)$$
$$+ P(\neg B)P(\neg E) P(\neg A \mid \neg B, \neg E)P(J \mid \neg A)P(M \mid \neg A)$$
$$= (0.9)(0.8)(0.1)(0.9)(0.7) + (0.9)(0.8)(0.9)(0.2)(0.1) = 0.05832$$
$$P(B=t \mid E=f,J=t,M=t) = \cfrac{0.04064}{0.04064 + 0.05832} = 0.4106709782$$
\end{soln}

\begin{soln}
\begin{equation*}
\begin{split}
P(B=t \mid E=t,J=t,M=t) &= P(B \mid E,J,M) \\
&= \cfrac{P(B, E, J, M)}{P( E, J, M)} \\
&= \cfrac{P(B, E, J, M)}{P(B, E, J, M) + P(\neg B, E, J, M)} \\
\end{split}
\end{equation*}
$$P(B, E, J, M) = P(B)P(E) P(A \mid B, E)P(J \mid A)P(M \mid A)$$
$$+ P(B)P( E) P(\neg A \mid B, E)P(J \mid \neg A)P(M \mid \neg A)$$
$$= (0.1)(0.2)(0.9)(0.9)(0.7) + (0.1)(0.2)(0.1)(0.2)(0.1) = 0.01138$$
$$P(\neg B, E, J, M) = P(\neg B)P(E) P(A \mid \neg B, E)P(J \mid A)P(M \mid A)$$
$$+ P(\neg B)P(E) P(\neg A \mid \neg B, E)P(J \mid \neg A)P(M \mid \neg A)$$
$$= (0.9)(0.2)(0.3)(0.9)(0.7) + (0.9)(0.2)(0.7)(0.2)(0.1) = 0.03654$$
$$P(B=t \mid E=f,J=t,M=t) = \cfrac{0.01138}{0.01138 + 0.03654} = 0.2374791319$$
\end{soln}

\newpage
\section{Chow-Liu Algorithm [25 pts]}
Suppose we wish to construct a directed graphical model for 3 features $X$, $Y$, and $Z$ using the Chow-Liu algorithm. We are given data from 100 independent experiments where each feature is binary and takes value $T$ or $F$. Below is a table summarizing the observations of the experiment:

\begin{table}[H]
        \centering
                \begin{tabular}{cccc}
                           $X$ & $Y$ & $Z$ & Count \\
                                \hline
                                T & T & T & 36 \\
                                \hline
                                T & T & F & 4 \\
                                \hline
                                T & F & T & 2 \\
                                \hline
                                T & F & F & 8 \\
                                \hline
                                F & T & T & 9 \\
                                \hline
                                F & T & F & 1 \\
                                \hline
                                F & F & T & 8 \\
                                \hline
                                F & F & F & 32 \\
                                \hline
                \end{tabular}
\end{table}

\begin{enumerate}
	\item Compute the mutual information $I(X, Y)$ based on the frequencies observed in the data. (5 pts)
        \begin{soln}
        $$I(X,Y) = \sum_{x \in \{ T, F \}} \sum_{y \in \{ T, F \}} P(x, y) log_2 (\cfrac{P(x,y)}{P(x)P(y)}$$
        $$= P(X=T,Y=T)log_2(\cfrac{P(X=T,Y=T)}{P(X=T)P(Y=T)}) + P(X=T,Y=F)log_2(\cfrac{P(X=T,Y=F)}{P(X=T)P(Y=F)})$$
        $$+ P(X=F,Y=T)log_2(\cfrac{P(X=F,Y=T)}{P(X=F)P(Y=T)}) + P(X=F,Y=F)log_2(\cfrac{P(X=F,Y=F)}{P(X=F)P(Y=F)})$$
        $$= 0.4 log_2(\cfrac{0.4}{0.5 * 0.5}) + 0.1 log_2(\cfrac{0.1}{0.5 * 0.5}) + 0.1 log_2(\cfrac{0.1}{0.5 * 0.5}) + 0.4 log_2(\cfrac{0.4}{0.5 * 0.5})$$
        $$I(X,Y) = 0.2780719051$$
        \end{soln}
	\item Compute the mutual information $I(X, Z)$ based on the frequencies observed in the data. (5 pts)
        \begin{soln}
        $$I(X,Z) = \sum_{x \in \{ T, F \}} \sum_{z \in \{ T, F \}} P(x, z) log_2 (\cfrac{P(x,z)}{P(x)P(z)}$$
        $$= P(X=T,Z=T)log_2(\cfrac{P(X=T,Z=T)}{P(X=T)P(Z=T)}) + P(X=T,Z=F)log_2(\cfrac{P(X=T,Z=F)}{P(X=T)P(Z=F)})$$
        $$+ P(X=F,Z=T)log_2(\cfrac{P(X=F,Z=T)}{P(X=F)P(Z=T)}) + P(X=F,Z=F)log_2(\cfrac{P(X=F,Z=F)}{P(X=F)P(Z=F)})$$
        $$= 0.38 log_2(\cfrac{0.38}{0.5 * 0.55}) + 0.12 log_2(\cfrac{0.12}{0.5 * 0.45}) + 0.17 log_2(\cfrac{0.17}{0.5 * 0.55}) + 0.33 log_2(\cfrac{0.33}{0.5 * 0.45})$$
        $$I(X,Y) = 0.1328449618$$
        \end{soln}
	\item Compute the mutual information $I(Z, Y)$ based on the frequencies observed in the data. (5 pts)
        \begin{soln}
        $$I(Z,Y) = \sum_{z \in \{ T, F \}} \sum_{y \in \{ T, F \}} P(z, y) log_2 (\cfrac{P(z,y)}{P(z)P(y)}$$
        $$= P(Z=T,Y=T)log_2(\cfrac{P(Z=T,Y=T)}{P(Z=T)P(Y=T)}) + P(Z=T,Y=F)log_2(\cfrac{P(Z=T,Y=F)}{P(Z=T)P(Y=F)})$$
        $$+ P(Z=F,Y=T)log_2(\cfrac{P(Z=F,Y=T)}{P(Z=F)P(Y=T)}) + P(Z=F,Y=F)log_2(\cfrac{P(Z=F,Y=F)}{P(Z=F)P(Y=F)})$$
        $$= 0.45 log_2(\cfrac{0.45}{0.55 * 0.5}) + 0.1 log_2(\cfrac{0.1}{0.55 * 0.5}) + 0.05 log_2(\cfrac{0.05}{0.45 * 0.5}) + 0.4 log_2(\cfrac{0.4}{0.45 * 0.5})$$
        $$I(Z, Y) = 0.3973126097$$
        \end{soln}
	\item Which undirected edges will be selected by the Chow-Liu algorithm as the maximum spanning tree? (5 pts)
        \begin{soln}
        We have 3 edges $(X,Y)$, $(X,Z)$ and $(Z,Y)$ and we select the largest mutual information edge $(Z,Y)$.\\
        Then we select the second largest mutual information edge $(X,Y)$. \\
        Finally we check the last edge $(X, Z)$, however, this creates a cycle in the tree so we do not add it to our MST. \\
        Therefore we end up with the MST with vertices $X$, $Y$ and $Z$, and edges $(Z,Y)$ and $(X,Y)$.
        \end{soln}
	\item Root your tree at node $X$, assign directions to the selected edges. (5 pts)
        \begin{figure}[H]
			\centering
			\includegraphics[width=0.3\textwidth]{images/q3_graph.png}
			\label{fig:gan_q1_loss}
		\end{figure}
\end{enumerate}

	\bibliography{hw6}
	\bibliographystyle{apalike}
\end{document}
